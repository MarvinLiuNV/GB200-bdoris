SETTINGS:
  name: HEALTH

VARS:
  CLUSTER_NAME: "<CLUSTER_NAME>"
  CONFIGURATION: "<CONFIGURATION>"
  TAGS: "<TAG>"
  SLURM_PARTITION: ""
  CONTAINER: ''
  MELLANOX_VISIBLE_DEVICES: ''
  CUDA_VISIBLE_DEVICES: ''
  NCCL_IB_HCA: ''
  NTASKS_PER_NODE: ''
  NUM_GPUS: ''
  NCCL_SOCKET_IFNAME: ''
  NCCL_MNNVL_ENABLE: ''
  CHECK_EXPRESSION: ''

JOBS:
  # NCCL All Reduce host isolation
  - $nccl_job[nccl_all_reduce]$ --NCCL_TESTS_SPLIT_MASK=0 --NCCL_CROSS_NIC=1 --NCCL_ALGO=ring
    --NCCL_IB_SPLIT_DATA_ON_QPS=0 --NCCL_IB_QPS_PER_CONNECTION=4 --NCCL_BUFFSIZE=4194304
    $slurm_all_hosts $group_1_host_filter
    $host_bw_analysis $benchmark_tag[NCCL_Health_Check]$

ALIAS:
  nccl_job benchmark minbytes=16G maxbytes=16G n=10 f=2 g=1 : jobs hpc nccl_tests --benchmark=$benchmark
    --minbytes=$minbytes --n=$n --f=$f --g=$g --w=50 --maxbytes=$maxbytes hpc_runner --runner=srun 
      jobs hpc hpc_configuration srun --container_image={CONTAINER} --rm=pyt_init.store cuda nccl --NCCL_SOCKET_IFNAME={NCCL_SOCKET_IFNAME}
    --NCCL_MNNVL_ENABLE={NCCL_MNNVL_ENABLE}
    --MELLANOX_VISIBLE_DEVICES={MELLANOX_VISIBLE_DEVICES} --NCCL_IB_GID_INDEX=3 --NCCL_IB_TC=96 --ntasks_per_node={NTASKS_PER_NODE}
    --mpi=pmix --num_gpus={NUM_GPUS} --CUDA_VISIBLE_DEVICES={CUDA_VISIBLE_DEVICES} --NCCL_DEBUG=warn --NCCL_NET_PLUGIN=none
    --NCCL_IB_HCA={NCCL_IB_HCA}

  # slurm jobs
  slurm_all_hosts id=1: jobs slurm_allocator --all_hosts=true --allocation_group_id=$id --partition={SLURM_PARTITION}

  # modifier
  group_1_host_filter: pairing_allocation pairing_rules k_node_grouper --k=1 --node_type=Host

  # analysis
  host_bw_analysis: analysis worker_oriented_metrics --expression='{CHECK_EXPRESSION}' --remove_worker_on_failure=true

  # tags
  benchmark_tag benchmark: tagging --cluster_name={CLUSTER_NAME} --configuration={CONFIGURATION} --benchmark=$benchmark --tags={TAGS}